{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec038c24-5f5f-40ff-928c-9ecfef480e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import AutoTokenizer\n",
    "from rlhfutils.data import preproc_wgpt, preproc_apf, preproc_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b256d4d8-ce12-4422-a5c8-9e1b17c935b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a92a1-d969-40f3-964c-8ae18a629242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rlcd = load_dataset(\"csv\", data_files=\"../rlcd-llama/simulated_data/simulated_preference_data_consolidated_helpful7b.csv\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "151c4bea-2e87-444b-bc36-7cc39dc98768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset webgpt_comparisons (/home/prasann/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
     ]
    }
   ],
   "source": [
    "webgpt = load_dataset(\"openai/webgpt_comparisons\", split=\"train\")\n",
    "webgpt = pd.DataFrame([preproc_wgpt(w) for w in webgpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3758cd3-5ffd-447e-8df0-e4b701afa61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/prasann/.cache/huggingface/datasets/lvwerra___parquet/lvwerra--stack-exchange-paired-1132fd48cbaac550/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "stack = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/reward\", split=\"train\")\n",
    "stack = stack.select(range(100000))\n",
    "stack = pd.DataFrame(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4121a181-b163-4ec4-8ae3-a5e588bee934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset alpaca_farm (/home/prasann/.cache/huggingface/datasets/tatsu-lab___alpaca_farm/alpaca_gpt4_preference/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Found cached dataset alpaca_farm (/home/prasann/.cache/huggingface/datasets/tatsu-lab___alpaca_farm/alpaca_human_preference/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 291.74it/s]\n"
     ]
    }
   ],
   "source": [
    "apfgpt = load_dataset(\"tatsu-lab/alpaca_farm\", 'alpaca_gpt4_preference')['preference']\n",
    "apfhum= load_dataset(\"tatsu-lab/alpaca_farm\", 'alpaca_human_preference')['preference']\n",
    "apfgpt = pd.DataFrame([preproc_apf(w) for w in apfgpt])\n",
    "apfhum = pd.DataFrame([preproc_apf(w) for w in apfhum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3d6729-25af-4c5f-8751-43a58160dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/prasann/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-37c6f75e35564d2a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "hh_train = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"train\")\n",
    "hh_train = pd.DataFrame([preproc_hh(w) for w in hh_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af1322bd-84cc-4a15-8c95-78034fc62569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That depends on the 9 year old, but if they like thrillers or mysteries then maybe something like “The Body in the Bog” or “The Boy Who Followed Ripley.” For more modern, female-written fiction I might recommend “Jane Eyre” or “Frankenstein.”\n",
      "\n",
      "Human: That sounds cool\n",
      "\n",
      "Assistant: Do you want some quotes?\n",
      "_____\n",
      " That depends on the 9 year old, but if they like thrillers or mysteries then maybe something like “The Body in the Bog” or “The Boy Who Followed Ripley.” For more modern, female-written fiction I might recommend “Jane Eyre” or “Frankenstein.”\n",
      "\n",
      "Human: That sounds cool\n",
      "\n",
      "Assistant: I’m a programmer, so I’m really into good programming texts, like “The Practice of Programming” or “JavaScript: The Good Parts.” But that might be a bit too much for a 9 year old, so\n"
     ]
    }
   ],
   "source": [
    "ind = 4\n",
    "print(hh_train['response_k'][ind])\n",
    "print(\"_____\")\n",
    "print(hh_train['response_j'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57512873-23fe-491e-b9ff-b4aa341a41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/prasann/Projects/tfr-decoding/llama/llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c4131c-55d7-47ad-900d-3164bf0a8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE I validated that RLCD doesn't have any input formatted stuff\n",
    "def rlcdmakeprefs (inpdf):\n",
    "    allres = []\n",
    "    for row in inpdf:\n",
    "        res = {}\n",
    "        res['question'] = row['instruction']\n",
    "        if row['preference']==1:\n",
    "            res['response_j'] = row['output_1']\n",
    "            res['response_k'] = row['output_2']\n",
    "        else:\n",
    "            res['response_j'] = row['output_2']\n",
    "            res['response_k'] = row['output_1']\n",
    "        allres.append(res)\n",
    "    return pd.DataFrame(allres).dropna().reset_index(drop=True)\n",
    "\n",
    "# take in processed df, given tokenizer, tokenize everything\n",
    "def tokall (pdf): \n",
    "    gtoks = []\n",
    "    btoks = []\n",
    "    for ind, row in pdf.iterrows():\n",
    "        gtoks.append(len(tokenizer(row['response_j']).input_ids))\n",
    "        btoks.append(len(tokenizer(row['response_k']).input_ids))\n",
    "    pdf['gtoks'] = gtoks\n",
    "    pdf['btoks'] = btoks\n",
    "    return pdf\n",
    "\n",
    "def lenbias (indf):\n",
    "    return (indf['gtoks']>indf['btoks']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f87090c-a2a1-4c5d-9aee-23a9f23e3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n",
      "tokenizing\n"
     ]
    }
   ],
   "source": [
    "print(\"processing\")\n",
    "rlcproc = rlcdmakeprefs(rlcd)\n",
    "print(\"tokenizing\")\n",
    "rlcproc = tokall(rlcproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88718152-4640-4abe-b4f2-0a247bff4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgptproc = tokall(webgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "086aaa69-49e9-4b66-8506-257113bbe3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5565430585350905"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenbias(wgptproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28890a0c-dc2b-48c0-9878-43df3394405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackproc = tokall(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "203e9ecb-d46c-4208-b0d6-7217e5a6610a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59698"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenbias(stackproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee3b59e1-3e93-47f5-8d17-09c0bde6b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "apfhumbproc = tokall(apfhum)\n",
    "apfgptproc = tokall(apfgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f49de53-c380-479c-a1ed-961a865b26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418429470642865\n",
      "0.535024650780608\n"
     ]
    }
   ],
   "source": [
    "print(lenbias(apfhumbproc))\n",
    "print(lenbias(apfgptproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1260771-81fe-46c6-8d86-04050d624bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhproc = tokall(hh_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae169927-f03e-4f37-887a-7cce82a12655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5828219459336147"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenbias(hhproc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
