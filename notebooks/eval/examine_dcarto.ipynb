{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf936fc-d7f0-4265-9eff-f9566695e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, stdev\n",
    "import matplotlib\n",
    "from rlhfutils.data import load_wgpt, augment_data, load_apfarm, load_stack, load_rlcd\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import random\n",
    "from statistics import mean, stdev\n",
    "from random import choices\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731933e5-1eed-49b3-9e2e-d1369a0a4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c93809-4601-4525-9f44-ea47ffc43695",
   "metadata": {},
   "outputs": [],
   "source": [
    "toker = AutoTokenizer.from_pretrained(\"../models/rewards/apffgoodcut2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c616e6-79be-4854-94dd-87d3cabbf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigint(acc, tot, runs):\n",
    "    sampdist = [1]*int(acc*tot) + [0]*int((1-acc)*tot)\n",
    "    mtots = []\n",
    "    for i in range(10000):\n",
    "        cs =  choices(sampdist, k=len(sampdist))\n",
    "        mtots.append(mean(cs))\n",
    "    return mean(mtots), stdev(mtots)\n",
    "\n",
    "def siginttwo(acc1, acc2, tot, runs):\n",
    "    sampdist = [1]*int(acc1*tot) + [0]*int((1-acc1)*tot)\n",
    "    sampdist2 = [1]*int(acc2*tot) + [0]*int((1-acc2)*tot)\n",
    "    mtots = []\n",
    "    for i in range(10000):\n",
    "        cs =  choices(sampdist, k=len(sampdist))\n",
    "        cs2 =  choices(sampdist2, k=len(sampdist2))\n",
    "        mtots.append(int(mean(cs) > mean(cs2)))\n",
    "    return mean(mtots), stdev(mtots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb166039-fd2b-4979-88c7-31cb313c4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "siginttwo(0.34, 0.30, 500, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6763b-daf2-4477-bde9-cc7370072c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation function to collect values into a list and sort by index\n",
    "def collect_and_sort(series):\n",
    "    return list(series.sort_index())\n",
    "\n",
    "def make_statdf(cdf):\n",
    "    # Group by 'uid' and aggregate\n",
    "    new_df = cdf.groupby('uid').agg({\n",
    "        'rew_j': collect_and_sort,\n",
    "        'rew_k': collect_and_sort\n",
    "    }).reset_index()\n",
    "    new_df['diffs'] = [[r['rew_j'][i] - r['rew_k'][i] for i in range(len(r['rew_j']))] for _, r in new_df.iterrows()]\n",
    "    return new_df\n",
    "\n",
    "def lenscat(idf):\n",
    "    return plt.hist2d(idf['vars'], idf['confs'], bins=(50, 50), norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "def lenheur(idf):\n",
    "    return  (idf['tj']>idf['tk']).mean()\n",
    "\n",
    "def alltoks(col):\n",
    "    atoks = []\n",
    "    for c in col:\n",
    "        atoks.extend(list(set(toker(c).input_ids)))\n",
    "    return [toker.decode(d) for d in atoks]\n",
    "\n",
    "def companalysis(indf, thresh):\n",
    "    good = Counter(alltoks(indf['rj']))\n",
    "    bad = Counter(alltoks(indf['rk']))\n",
    "\n",
    "    total = good+bad\n",
    "    total = {token: count for token, count in total.items() if count >= thresh}\n",
    "    \n",
    "    # Compute the conditional probability P(Class|Token) for each token and each class\n",
    "    good_token_prob = {token: good.get(token, 0) / total[token] for token in total}\n",
    "    bad_token_prob = {token: bad.get(token, 0) / total[token] for token in total}\n",
    "    \n",
    "    # Find top tokens for each class\n",
    "    top_good_tokens = sorted(good_token_prob.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_bad_tokens = sorted(bad_token_prob.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print(\"Top tokens with probabilities for 'good' class:\")\n",
    "    for token, prob in top_good_tokens:\n",
    "        print(f\"{token}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"Top tokens with probabilities for 'bad' class:\")\n",
    "    for token, prob in top_bad_tokens:\n",
    "        print(f\"{token}: {prob:.4f}\")\n",
    "\n",
    "def lenscat(idf):\n",
    "    plt.hist2d(idf['vars'], idf['confs'], bins=(50, 50), norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "def lenheur(idf):\n",
    "    return  (idf['tj']>idf['tk']).mean()\n",
    "\n",
    "def alltoks(col):\n",
    "    atoks = []\n",
    "    for c in col:\n",
    "        atoks.extend(list(set(toker(c).input_ids)))\n",
    "    return [toker.decode(d) for d in atoks]\n",
    "\n",
    "def companalysis(indf, thresh):\n",
    "    good = Counter(alltoks(indf['rj']))\n",
    "    bad = Counter(alltoks(indf['rk']))\n",
    "\n",
    "    total = good+bad\n",
    "    total = {token: count for token, count in total.items() if count >= thresh}\n",
    "    \n",
    "    # Compute the conditional probability P(Class|Token) for each token and each class\n",
    "    good_token_prob = {token: good.get(token, 0) / total[token] for token in total}\n",
    "    bad_token_prob = {token: bad.get(token, 0) / total[token] for token in total}\n",
    "    \n",
    "    # Find top tokens for each class\n",
    "    top_good_tokens = sorted(good_token_prob.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_bad_tokens = sorted(bad_token_prob.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print(\"Top tokens with probabilities for 'good' class:\")\n",
    "    for token, prob in top_good_tokens:\n",
    "        print(f\"{token}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"Top tokens with probabilities for 'bad' class:\")\n",
    "    for token, prob in top_bad_tokens:\n",
    "        print(f\"{token}: {prob:.4f}\")\n",
    "        \n",
    "@dataclass\n",
    "class tmpdata: \n",
    "    mix_ratio: str\n",
    "    rand_ratio:  str\n",
    "\n",
    "def procn(ntmp, tdset):\n",
    "    try:\n",
    "        ntmp['isrand'] = [tdset[r['uid']]['isrand'] for _, r in ntmp.iterrows()]\n",
    "    except: \n",
    "        print(\"no randd\")\n",
    "    ntmp['rj'] = [tdset[r['uid']]['response_j'] for _, r in ntmp.iterrows()]\n",
    "    ntmp['rk'] = [tdset[r['uid']]['response_k'] for _, r in ntmp.iterrows()]\n",
    "    ntmp['tj'] = [len(toker(r['rj']).input_ids) for _, r in ntmp.iterrows()]\n",
    "    ntmp['tk'] = [len(toker(r['rk']).input_ids) for _, r in ntmp.iterrows()]\n",
    "    return ntmp\n",
    "    \n",
    "def getprocddata(statdf, dset, da=False):\n",
    "    print(\"load \", dset)\n",
    "    # if we're doing WGPT\n",
    "    if dset==\"wgpt\":\n",
    "        train_dataset = load_wgpt()[0]\n",
    "    elif dset==\"apf\":\n",
    "        train_dataset = load_apfarm(\"gpt4\")[0]\n",
    "    elif dset==\"rlcd\":\n",
    "        train_dataset = load_rlcd()[0]\n",
    "    elif dset==\"stack\":\n",
    "        train_dataset = load_stack()[0]\n",
    "    if da:\n",
    "        train_dataset = augment_data(train_dataset, tmpdata(**{\"mix_ratio\":0, \"rand_ratio\":0.2}))\n",
    "    train_dataset = augment_data(train_dataset, tmpdata(**{\"mix_ratio\":0, \"rand_ratio\":0}))\n",
    "    print(\"start processing, will take a minute\")\n",
    "    return procn(statdf, train_dataset)\n",
    "\n",
    "cartopaths = {\n",
    "    \"wgpt\":\"../trl-general/carto_outs/webgpt_carto.jsonl\",\n",
    "    \"stack\":\"../trl-general/carto_outs/stack_carto.jsonl\",\n",
    "    \"rlcd\":\"../trl-general/carto_outs/rlcd_carto.jsonl\",\n",
    "    \"apf\":\"../trl-general/carto_outs/apf_carto.jsonl\"\n",
    "}\n",
    "\n",
    "def load_carto(dsname, da=False, getprocs=True):\n",
    "    if getprocs:\n",
    "        cartdf = pd.read_json(cartopaths[dsname], orient='records', lines=True)\n",
    "    else:\n",
    "        cartdf = pd.read_json(dsname, orient='records', lines=True)\n",
    "    ndf = make_statdf(cartdf)\n",
    "    ndf['confs'] = [mean(m[1:]) for m in ndf['diffs']]\n",
    "    ndf['vars'] = [stdev(m[1:]) for m in ndf['diffs']]\n",
    "    if getprocs:\n",
    "        return getprocddata(ndf, dsname, da)\n",
    "    else:\n",
    "        return ndf\n",
    "\n",
    "def heurscatter(dset, interv, col='confs'):\n",
    "    bottom = int(min(dset[col]))\n",
    "    top = int(max(dset.confs))\n",
    "    xvals = []\n",
    "    yvals = []\n",
    "    dist = []\n",
    "    for i in range(1, int((top-bottom)/interv)):\n",
    "        curx = bottom + (interv)*i\n",
    "        tmpslice = dset.loc[(dset[col]<curx) & (dset[col]>curx-interv)]\n",
    "        xvals.append(curx)\n",
    "        yvals.append(lenheur(tmpslice))\n",
    "        dist.append(len(tmpslice))\n",
    "    return xvals, yvals, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001603f1-b93b-48a8-901b-6808d9c38902",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcd = load_carto(\"rlcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebbee4-07ac-4000-908f-ee1bf3b32a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgpt = load_carto(\"wgpt\")\n",
    "stack = load_carto(\"stack\")\n",
    "#rlcd = load_carto(\"rlcd\")\n",
    "#apfarm = load_carto(\"apf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bff2d-4852-4562-88e6-26f9e4835301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abf8a0-ff69-4720-be3c-148daaadff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cartodata.pkl', 'wb') as f:\n",
    "    pickle.dump((wgpt, stack, rlcd), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113187f-46d7-41cc-8c4f-f19758a59d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cartodata.pkl', 'rb') as f:\n",
    "    (w, s, r) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9ee91-b9da-47fc-9eee-b8021ba3653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotscatter(dset, interv, col, title):\n",
    "    x, y, d = heurscatter(dset, interv, col)\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.title.set_title(title) \n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Length Heuristic')\n",
    "    ax.set_title(title)\n",
    "    ax.scatter(x, y)\n",
    "    #plt.savefig(title+col+\"DClenheur.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "f = plotscatter(stack, 0.1, 'confs', \"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162c0e5-609b-4009-8e17-83e88237673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotscatter(dset, interv, col, title, ax):\n",
    "    x, y, d = heurscatter(dset, interv, col)\n",
    "    \n",
    "    # Scale the d values to use for point size. You can adjust the scaling factor.\n",
    "    sizes = [val * 10 for val in d]  # adjust the factor (here, 50) as per your data and preferences\n",
    "    \n",
    "    sns.scatterplot(x=x, y=y, size=sizes, sizes=(50, 1000), ax=ax, legend=False)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Length Heuristic')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Setting the style for a prestigious machine learning conference\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")  # or \"paper\" or \"talk\", depends on how big you want things\n",
    "\n",
    "# Create a figure and axes for the three scatter plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot each scatter plot on the individual axes\n",
    "plotscatter(w, 0.2, 'confs', \"W-GPT\", axs[0])\n",
    "plotscatter(s, 0.5, 'confs', \"Stack\", axs[1])\n",
    "plotscatter(r, 1, 'confs', \"RLCD\", axs[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "# Uncomment the next line if you want to save the figure\n",
    "plt.savefig(\"carto_scatters.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893a9aa-9c31-484d-b87d-6ebc0ec677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plotscatter(stack, 0.1, 'confs', \"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cd403-c2be-4ae0-9165-4c7ea2e05b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get versions with both cuts and only 1\n",
    "dsets = {\n",
    "    \"webgpt\":wgpt,\n",
    "    \"apf\":apfarm,\n",
    "    \"rlcd\":rlcd,\n",
    "    \"stack\":stack\n",
    "}\n",
    "cuts = {\n",
    "    \"webgpt\":[-0.2, 0.4],\n",
    "    \"apf\":[-0.3, 0.3],\n",
    "    \"rlcd\":[-1, 2],\n",
    "    \"stack\": [-1,  1]\n",
    "}\n",
    "for c in cuts.keys():\n",
    "    tmp = dsets[c]\n",
    "    #tmp = tmp[tmp.confs>cuts[c][0]]\n",
    "    #tmp.uid.to_json(\"../trl-general/truncvals/\"+c+\"bad.json\", lines=True,  orient='records')\n",
    "    tmp = tmp[tmp.confs<cuts[c][1]]\n",
    "    tmp.uid.to_json(\"../trl-general/truncvals/\"+c+\"good.json\", lines=True,  orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bf160-60d6-40c0-8c11-834dd6ef3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(apfarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4385f-7309-4be7-9c71-f3ab6f359289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = apfarm\n",
    "tmp = tmp[(tmp.confs>0.25)|(tmp.confs<-0.25)]\n",
    "print(len(tmp), \"/\", len(apfarm))\n",
    "lenscat(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01f2ba-8bca-4573-a8f4-afc71a3493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = list(pd.read_json(\"../trl-general/truncvals/rlcdboth.json\", orient='records', lines=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca27a98-0cc7-4616-b14b-d263ae0a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = list(tmp['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf23bd-cb18-4b1f-a227-ae05e06893e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91287656-20ce-4ac3-b7ca-bcb1ec74fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenheur(rlcd[rlcd.confs<0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4993cbf-fab7-421d-a3eb-88e20da19a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenheur(stack.loc[(stack.confs<0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e23c9-d7ee-4972-88bb-813d64e43196",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d5fce-0a22-483d-a467-91436b711ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1134617-9523-4027-b3f3-5d75a2fb3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonly = load_carto(\"../trl-general/carto_outs/.jsonl\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24712099-ee9d-4a32-bf34-a0c5ff22e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(lonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bbe52-6877-46b5-84ad-fef6ebfedfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonly = load_carto(\"../trl-general/carto_outs/wgptbothandda.jsonl\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98780c-4fa3-44b9-b12a-109a65ed70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(lonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf92125-df7f-4a60-930e-d0e6a74c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcdbase = load_carto(\"../trl-general/carto_outs/rlcd_carto.jsonl\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d2d13-b510-43b6-a6db-90f8382ee97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.read_json(\"../trl-general/carto_outs/leftonlyv3rm.jsonl\", orient='records', lines=True)\n",
    "sdf = make_statdf(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dde6aa-44af-4c8d-93e1-e816b6808529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsanity = load_carto(\"../trl-general/carto_outs/midcutsanity.jsonl\", False, False)\n",
    "rsanity = load_carto(\"../trl-general/carto_outs/rlcdgoodcutv3.jsonl\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb947d1-f596-495e-9177-915ad2a73660",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(rsanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb2d5d-f7ba-48dd-a3f5-247a4b668bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsani.vars.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd743ec-8e6e-4bff-a9c0-5ad7caeae32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsanity.vars.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c88cad-5b50-4128-9531-d258ed3f0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcdbase.confs.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe4294-50f7-4c55-8aa6-f50acf985422",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanuids = pd.read_json(\"../trl-general/truncvals/rlcdbothcutminisanity.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad2365-d3f2-4946-a031-3d3d23298ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(rlcdbase[rlcdbase.uid.isin(sanuids[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd8b1c-044d-4057-a0f9-feb703cca09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(rsanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d9f94-aff9-49ff-89c6-cafb37e900f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "midt.vars.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869947-ad76-4e55-aa2f-c20b27ce4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.uid.to_json(\"../trl-general/truncvals/\"+\"leftonly.json\", lines=True,  orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc10f61-067b-494d-a410-cb9bc54a633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = stack\n",
    "tmp = tmp[tmp.confs<1]\n",
    "tmp = tmp[tmp.confs>-1]\n",
    "print(len(tmp))\n",
    "lenscat(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2ba5c-e3e6-4c28-932f-94b32adc8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncuids = tmp.uid\n",
    "truncuids.to_json(\"../trl-general/truncvals/rlcdbadhoriz.json\", lines=True,  orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186806f-04c8-4c2a-9606-231e0af03a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files where I'm storing truncvals\n",
    "fnames = ['wgboth', 'wgbadonly', 'rlcdbadonly', 'rlcdboth', 'apfboth', 'apfbadonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347decd-d0b3-4566-b6b0-f8d09016ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(pd.read_json(\"../trl-general/truncvals/webgptboth.json\", lines=True, orient='records')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d893388-5703-43d5-9838-424a7e10a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#companalysis(hard[hard.isrand==0], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157574aa-68c9-4fe2-8d3c-ea142459d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(ndf[ndf.confs>ndf.vars*2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cebb17-f85c-4204-848f-a8fc35cd9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e78b4-8140-4809-9b47-e8ed6d81cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when skipping first \n",
    "easy = ndf[ndf['confs']-4>(ndf['vars']*1.6)]\n",
    "ambig = ndf[ndf['confs']-4<(ndf['vars']*1.6)]\n",
    "hard = ambig[ambig['confs']-10<(ambig['vars']*-0.5)]\n",
    "ambig = ambig[ambig['confs']-10>(ambig['vars']*-0.5)]\n",
    "# when not skipping first\n",
    "# easy = ndf[ndf['confs']-2.5>(ndf['vars']*1.6)]\n",
    "# ambig = ndf[ndf['confs']-2.5<(ndf['vars']*1.6)]\n",
    "# hard = ambig[ambig['confs']-10<(ambig['vars']*-2)]\n",
    "# ambig = ambig[ambig['confs']-10>(ambig['vars']*-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753902a-3c81-4c0a-895f-0d1456300618",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenheur(hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f8711-8b88-419a-acf4-db88c2bb01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ndf['vars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19c361-e2ce-4574-a12f-3f78f88222a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard.isrand.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028692f-3753-465c-b843-9dab0927fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(ndf[(ndf.tj-ndf.tk)>40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894cfbb-4645-4c5a-9540-d660bf927e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.loc[0].rew_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99a062-6632-4555-9cf9-fe791ecfc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 4\n",
    "print(easy.iloc[ind]['rj'])\n",
    "print(\"____\")\n",
    "print(easy.iloc[ind]['rk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31b704-e2b9-4fd0-9528-6f3da96b4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "companalysis(hard, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fac6f-5cbc-4e8e-80ee-a8c4bfe4e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57698d-1b97-4e60-938a-ed3f29de0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenheur(ambig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ffc69-2f2f-49ee-865f-c2498aefb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenscat(hard[ndf['tj']>ndf['tk']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad67f66-29c0-418a-ae8c-16ef9e65eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ndf['vars'], ndf['confs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281e9cc-2b57-4e50-8cb9-2c29c139fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = ndf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b767b9-fec5-4308-b558-8f34436b4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(ndf['vars'], ndf['confs'], bins=(50, 50), norm=matplotlib.colors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57de7d-f576-44d2-90b2-d1ee1b0a8025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
