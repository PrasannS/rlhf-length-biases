{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f2224-d5a9-467b-82a6-27d17c4079ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/prasanns/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec443df-2c8d-4ece-9478-4722eb1e81b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('wikipedia', \"20220301.en\", split='train', streaming=True)\n",
    "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0896c8be-aa2e-428a-95f5-28aee32a8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = list(dataset.take(30000))\n",
    "ndata = Dataset.from_list(nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b4e3b-a1b6-42e7-a913-5192464ea14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset[6]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "472fa140-eddb-41d6-9dfb-6528ce53f995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/u/prasanns/.cache/huggingface/datasets/jstet___csv/jstet--quotes-500k-ede96e03d28fbb72/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 220.75it/s]\n"
     ]
    }
   ],
   "source": [
    "quotesdata = load_dataset(\"jstet/quotes-500k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a62dc52d-42f2-4bd4-9477-ee0bc291c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/prasanns/.cache/huggingface/datasets/jstet___csv/jstet--quotes-500k-ede96e03d28fbb72/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-602235f45088ee8c.arrow\n"
     ]
    }
   ],
   "source": [
    "TOTDATA = 60000\n",
    "quotesdata = quotesdata['train'].shuffle(seed=0)\n",
    "quotesdata = quotesdata.select(range(TOTDATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540d5979-6855-4db0-9474-112b880c26a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': \"The rose, however, made us girls somewhat fainthearted, because it really was something we felt mattered, the white bridal dream with the wedding bouquet and the kiss from the man who was to be ours forever. But then Laura said that the lady who had given it to us had gotten divorced only five years later. And since many of our parents were also divorce, if indeed they had ever been married at all, that dream clearly wasn't worth our time.\",\n",
       " 'author': 'Janne Teller',\n",
       " 'category': 'marriage, nothing'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotesdata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548c0fe1-3b87-4952-9bfb-887619b68c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"facebook/opt-125m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb276ba2-8676-4745-b703-d6ed729cc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(modelname, device_map=0)\n",
    "model.eval()\n",
    "toker = AutoTokenizer.from_pretrained(modelname, padding_side='left')\n",
    "\n",
    "toker.max_length=512\n",
    "toker.padding_size='left'\n",
    "toker.pad_token = toker.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d1aaa6-e246-4af9-9f87-c15cffa3bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_quotes(exs):\n",
    "    inps = []\n",
    "    for e in exs:\n",
    "        s = e['author']+ \": \" + e['quote']\n",
    "        if len(s)<200:\n",
    "            inps.append(s)\n",
    "    return inps\n",
    "\n",
    "def proc_wiki(exs):\n",
    "    inps = []\n",
    "    for e in exs:\n",
    "        svals = e['text'].split(\".\")\n",
    "        pots = []\n",
    "        for s in svals: \n",
    "            if len(s)>20 and len(s)<200:\n",
    "                pots.append(s.strip())\n",
    "        inps.extend(pots[:3])\n",
    "    print(len(inps))\n",
    "    return inps\n",
    "        \n",
    "def generate_trunc(inputs, trunc, model, mbatch_size=4, top_p=0.9, temp=0.4):\n",
    "    newinps = []\n",
    "    corrgens = []\n",
    "    for inp in inputs: \n",
    "        newinps.append(toker.decode(toker(inp).input_ids[:-(trunc+1)], skip_special_tokens=True))\n",
    "        corrgens.append(toker.decode(toker(inp).input_ids[-(trunc+1):], skip_special_tokens=True))\n",
    "    newgens = []\n",
    "    for i in tqdm(range(0, len(newinps), mbatch_size)):\n",
    "        inps = toker(newinps[i:i+mbatch_size], padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        newgens.extend(model.generate(**inps, max_new_tokens=trunc+1, do_sample=True, top_p=top_p, temperature=temp))\n",
    "    return toker.batch_decode(newgens, skip_special_tokens=True), corrgens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb136151-773b-4fcc-9e33-bd83a7045daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84757\n"
     ]
    }
   ],
   "source": [
    "procd = proc_wiki(ndata.select(range(len(ndata))))\n",
    "# procd = proc_quotes(quotesdata.select(range(100)))\n",
    "# gtrunc, golds = generate_trunc(procd, 3, model, 4, 0.9, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d095d530-4cef-439b-95cd-7d180131b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf = pd.DataFrame({'outputs':procd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "036a95e9-0b30-4b9a-8717-0410ba5fff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdata = Dataset.from_pandas(datadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33fcc76c-1d54-4021-b8ad-dc64ec2cccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "wdata.save_to_disk(\"../../data/wikidatasft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e0d6c7cc-ffcb-4221-965e-bd57c5d719d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'category'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "60a094cd-3741-45ec-afcd-bf7139d50fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:  a poet arranges meaning in the sounds.: A versifier arranges sounds\n",
      "GOLD: ifier arranges sounds\n",
      "GEN:  a poet arranges meaning in the sounds.: A versification of the poetry\n"
     ]
    }
   ],
   "source": [
    "ind = 9\n",
    "print(\"ORIGINAL: \"+procd[ind])\n",
    "print('GOLD: '+golds[ind])\n",
    "print(\"GEN: \"+gtrunc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f005b5c-7bd0-481b-96e8-e24a3a257cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.81k/7.81k [00:00<00:00, 13.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/plain_text to /u/prasanns/.cache/huggingface/datasets/parquet/plain_text-730f38e7e31e7fd3/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                                                      | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                      | 0.00/21.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  31%|█████████████████████████████████▉                                                                           | 6.53M/21.0M [00:00<00:00, 65.2MB/s]\u001b[A\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21.0M/21.0M [00:00<00:00, 96.2MB/s]\u001b[A\n",
      "Downloading data files:  50%|███████████████████████████████████████████████████████                                                       | 1/2 [00:00<00:00,  1.36it/s]\n",
      "Downloading data:   0%|                                                                                                                      | 0.00/20.5M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  35%|█████████████████████████████████████▊                                                                       | 7.10M/20.5M [00:00<00:00, 70.7MB/s]\u001b[A\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20.5M/20.5M [00:00<00:00, 97.6MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.37it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 147.43it/s]\n",
      "                                                                                                                                                                         \r"
     ]
    },
    {
     "ename": "ExpectedMoreSplits",
     "evalue": "{'unsupervised'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce_redownload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/datasets/load.py:1809\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1820\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1821\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/datasets/builder.py:1022\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     dl_manager\u001b[38;5;241m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[43mverify_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;241m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/datasets/utils/info_utils.py:91\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExpectedMoreSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(expected_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(recorded_splits)))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedSplits(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mset\u001b[39m(recorded_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(expected_splits)))\n",
      "\u001b[0;31mExpectedMoreSplits\u001b[0m: {'unsupervised'}"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"imdb\",download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b394f-49a3-4bbe-9c8f-e1446584ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
