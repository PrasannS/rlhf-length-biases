{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66644557-0a5d-4984-92dc-631aeb6ddca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a notebook to do experiment requested by Nathan for checking token-wise length bias of rewards \n",
    "# (my guess: we'll see a lot at smaller scales, not so much at bigger)\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from rlhfutils.data import qaform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ab8ccb-1add-43e7-9e8f-23c5d5709b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "# UltraRM format so that we don't need to re-run a ton of times for each unique input\n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f09da245-ece8-48df-baf1-742d7f9319db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhrlhf_preproc(inp, out):\n",
    "    resp = inp\n",
    "    if \"###Human: \" not in inp: \n",
    "        resp = \"###Human: \"+inp\n",
    "    if \"###Assistant: \" not in out: \n",
    "        resp = resp+\" ###Assistant: \"\n",
    "    resp = resp+out\n",
    "    return resp\n",
    "    \n",
    "def progress_causalrm(inps, mod, tok, cache):\n",
    "    cache.clear()\n",
    "    inps.reverse()\n",
    "    scores = []\n",
    "    # TODO test to see if batching is needed, currently this assumes no batching\n",
    "    for i in tqdm(range(len(inps))):\n",
    "        #print(inps[i])\n",
    "        # switch batck to original HHRLHF format (TODO this is weird)\n",
    "        question, answer = inps[i]\n",
    "        question = question.replace(\"###Assistant:\", \"\\n\\nAssistant:\").replace(\"###Human:\", \"\\n\\nHuman:\")\n",
    "        question = (question + \"\\n\\nAssistant:\").strip()\n",
    "        tokrewards = None\n",
    "        # used cached full response if we can\n",
    "        for k in cache.keys():\n",
    "            if question in k: \n",
    "                tokrewards = cache[question]\n",
    "        modinps = tok(question+answer, return_tensors='pt')\n",
    "        \n",
    "        if tokrewards==None: \n",
    "            modinps = modinps.to(mod.device)\n",
    "            # first one in the cache\n",
    "            cache[question] = mod(**modinps).tolist()[0]\n",
    "            tokrewards = cache[question]\n",
    "        if i==0:\n",
    "            print(question+answer)\n",
    "            #print(len(modinps))\n",
    "            #print(len(tokrewards))\n",
    "            \n",
    "        # get the token score based on length of stuff\n",
    "        scores.append(tokrewards[len(modinps.input_ids[0])-1])   \n",
    "        #print(len(tokrewards))\n",
    "        #print(len(modinps.input_ids[0])-1)\n",
    "    scores.reverse()\n",
    "    return scores\n",
    "\n",
    "def progress_oasst(inps, mod, tok):\n",
    "    scores = []\n",
    "    # TODO test to see if batching is needed\n",
    "    for i in tqdm(range(len(inps))):\n",
    "        #print(inps[i])\n",
    "        # switch batck to original HHRLHF format (TODO this is weird)\n",
    "        question, answer = inps[i]\n",
    "        question = question.replace(\"###Assistant:\", \"\\n\\nAssistant:\").replace(\"###Human:\", \"Human:\")\n",
    "        # question = (question + \"\\n\\nAssistant:\").strip()\n",
    "        question = question.strip()+\"\\n\\nAssistant: \"\n",
    "        if i==0:\n",
    "            print(question, answer)\n",
    "        modinps = tokenizer(question, answer, return_tensors='pt').to(mod.device)\n",
    "        scores.append(mod(**modinps).logits[0].cpu().detach())\n",
    "    return scores\n",
    "\n",
    "def load_rm(rmname): \n",
    "    # TODO maybe offload input formatting into here as well\n",
    "    if rmname==\"weqweasdas/hh_rlhf_rm_open_llama_3b\":\n",
    "        rm_tokenizer = AutoTokenizer.from_pretrained(\"weqweasdas/hh_rlhf_rm_open_llama_3b\")\n",
    "        rm_pipe = pipeline(\n",
    "          \"sentiment-analysis\",\n",
    "          model=\"weqweasdas/hh_rlhf_rm_open_llama_3b\",\n",
    "          device=1,\n",
    "          tokenizer=rm_tokenizer,\n",
    "          model_kwargs={\"torch_dtype\": torch.bfloat16, 'attn_implementation':\"flash_attention_2\"}\n",
    "        )\n",
    "        pipe_kwargs = {\n",
    "          \"return_all_scores\": True,\n",
    "          \"function_to_apply\": \"none\",\n",
    "          \"batch_size\": 4\n",
    "        }\n",
    "        def progress_pipe(inps, kwargs, pipe): \n",
    "            chunk=16\n",
    "            results = []\n",
    "            print(inps[0])\n",
    "            for i in tqdm(range(0, len(inps), chunk)):\n",
    "                results.extend(pipe(inps, **kwargs))\n",
    "            return results\n",
    "        return lambda texts: [output[0][\"score\"] for output in progress_pipe([hhrlhf_preproc(i, o) for i, o in texts], pipe_kwargs, rm_pipe)]\n",
    "    if rmname==\"OpenAssistant/reward-model-deberta-v3-large-v2\":\n",
    "        rank_model, toker = AutoModelForSequenceClassification.from_pretrained(rmname, device_map=1), AutoTokenizer.from_pretrained(rmname)\n",
    "        return lambda texts: progress_oasst(texts, rank_model, toker)\n",
    "    if rmname==\"openbmb/UltraRM-13b\":\n",
    "        # load in models\n",
    "        rtoker = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\")\n",
    "        rmodel = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\", device_map=1, torch_dtype=torch.bfloat16)\n",
    "        rmodel.eval()\n",
    "        # cache full rewards (tokenwise) for the longest seqeunce (reverse the passing order), then we can pull from that \n",
    "        reward_cache = {}\n",
    "        \n",
    "        return lambda texts: progress_causalrm(texts, rmodel, rtoker, reward_cache)\n",
    "    if \"/u/prasanns/research/rlhf-length-biases/models/rewards\" in rmname:\n",
    "        rm_tokenizer = AutoTokenizer.from_pretrained(rmname)\n",
    "        rm_pipe = pipeline(\n",
    "          \"sentiment-analysis\",\n",
    "          model=rmname,\n",
    "          device=1,\n",
    "          tokenizer=rm_tokenizer,\n",
    "          model_kwargs={\"torch_dtype\": torch.bfloat16, 'attn_implementation':\"flash_attention_2\"}\n",
    "        )\n",
    "        pipe_kwargs = {\n",
    "          \"return_all_scores\": True,\n",
    "          \"function_to_apply\": \"none\",\n",
    "          \"batch_size\": 32\n",
    "        }\n",
    "        def progress_pipe(inps, kwargs, pipe): \n",
    "            chunk=16\n",
    "            results = []\n",
    "            print(inps[0])\n",
    "            for i in tqdm(range(0, len(inps), chunk)):\n",
    "                results.extend(pipe(inps, **kwargs))\n",
    "            return results\n",
    "        return lambda texts: [output[0][\"score\"] for output in progress_pipe([qaform(i, o) for i, o in texts], pipe_kwargs, rm_pipe)]\n",
    "\n",
    "def get_token_inps(inputs, toker, gap=1, flat=True):\n",
    "    oldinps = None\n",
    "    # only work with output part of input tuples (TODO this may need adaptation for sanity check)\n",
    "    if len(inputs[0])==2: \n",
    "        oldinps = inputs\n",
    "        inputs = [inputs[i][1] for i in range(len(inputs))]\n",
    "    finlist = []\n",
    "    toklists = [toker(inp).input_ids for inp in inputs]\n",
    "    for toks in toklists: \n",
    "        tmp = []\n",
    "        for ind in range(gap, len(toks), gap): \n",
    "            tmp.append(toker.decode(toks[:ind+1], skip_special_tokens=True))\n",
    "        if (len(toks)%gap)!=0:\n",
    "            tmp.append(toker.decode(toks, skip_special_tokens=True))\n",
    "        finlist.append(tmp)\n",
    "    if oldinps: \n",
    "        # we want something we can use directly\n",
    "        if flat: \n",
    "            rlens = [len(finlist[i]) for i in range(len(finlist))]\n",
    "            inps = []\n",
    "            outs = []\n",
    "            for i in range(len(oldinps)): \n",
    "                inps.extend([oldinps[i][0]]*rlens[i])\n",
    "                outs.extend(finlist[i])\n",
    "            rlens = [sum(rlens[:i+1]) for i in range(len(rlens))]\n",
    "            return [0]+rlens, [(inps[i], outs[i]) for i in range(len(outs))]\n",
    "        # get the corrected reprocessed thing\n",
    "        return [(oldinps[i][0], finlist[i]) for i in range(len(finlist))]\n",
    "    return finlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e3bd8b5-7906-435b-a5d8-76045789f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_hh(ex): \n",
    "    ex['chosen'] = ex['chosen'].replace(\"\\n\\n\", \" ###\")\n",
    "    ex['rejected'] = ex['rejected'].replace(\"\\n\\n\", \" ###\")\n",
    "    return ex\n",
    "\n",
    "def pp_uf(ex):\n",
    "    ex['chosen'] = \"###Human: \"+ex['question'].strip()+\" ###Assistant: \"+ex['response_j']\n",
    "    ex['rejected'] = \"###Human: \"+ex['question'].strip()+\" ###Assistant: \"+ex['response_k']\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e164e18-f041-44d6-8afa-f2da9da7a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/prasann/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-37c6f75e35564d2a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading cached processed dataset at /home/prasann/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-37c6f75e35564d2a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a0c4ef1f168e4156_*_of_00010.arrow\n"
     ]
    }
   ],
   "source": [
    "# Code for loading in data\n",
    "hh_train = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"test\").map(pp_hh, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f88216-1a89-45ec-a179-1533e59e689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /scratch/cluster/prasanns/research/rlhf-length-biases/data/ultra/ultrafeeddiff/cache-5898f0ebaad14ffe.arrow\n",
      "Loading cached processed dataset at /scratch/cluster/prasanns/research/rlhf-length-biases/data/ultra/ultrafeeddiff/cache-45b182770748902d.arrow\n",
      "Loading cached processed dataset at /scratch/cluster/prasanns/research/rlhf-length-biases/data/ultra/ultrafeeddiff/cache-472f00fcfc1cacab.arrow\n"
     ]
    }
   ],
   "source": [
    "ultrafeediff = Dataset.load_from_disk(\"../../data/ultra/ultrafeeddiff/\").shuffle(seed=0).select(range(1000)).filter(lambda ex: ex['tokj']<150).map(pp_uf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0ab35-91ad-4de9-97f2-060f9f7bcd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ultrafeediff['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3f4e232-c371-4aeb-b6b1-464dc9cdb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "rewardmodels = [\"OpenAssistant/reward-model-deberta-v3-large-v2\", \"weqweasdas/hh_rlhf_rm_open_llama_3b\", \"openbmb/UltraRM-13b\", \"/u/prasanns/research/rlhf-length-biases/models/rewards/bow/expbow50\", \n",
    "               \"/u/prasanns/research/rlhf-length-biases/models/rewards/bow/lenevenrm\"]\n",
    "ind = -1\n",
    "rmstr = rewardmodels[ind]\n",
    "# TODO maybe we should use the same tokenizer across all of these? Aligning between models is gonna be a bit weird\n",
    "# TODO set this back to 0\n",
    "tokenizer = AutoTokenizer.from_pretrained(rewardmodels[ind])\n",
    "model = load_rm(rmstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff8ea12e-ed3c-406a-8e1b-1f8547ee044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoretexts(texts, mod, gap=5): \n",
    "    test_inps = [x.rsplit(\"###Assistant: \", 1) for x in texts]\n",
    "    print(test_inps[0])\n",
    "    values, data = get_token_inps(test_inps, tokenizer, gap)\n",
    "    scos = model(data)\n",
    "    nscos = [scos[values[i-1]:values[i]] for i in range(1, len(values))]\n",
    "    return nscos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77b8b2fa-e22a-4412-9a9c-f44f05305bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\\nQ: [61, 35, -86, -38, 58, -9, 78]\\nA: ', 'The filtered list is: [61, 35, -86, -38, 58, -9]']\n",
      "Question: ###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\n",
      "Q: [61, 35, -86, -38, 58, -9, 78]\n",
      "A: \n",
      "\n",
      "Answer: The filtered list is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                          | 0/30 [00:00<?, ?it/s]/u/prasanns/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      " 33%|███████████████████████████████████████████████████████████                                                                                                                      | 10/30 [00:10<00:21,  1.09s/it]/u/prasanns/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:32<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\\nQ: [61, 35, -86, -38, 58, -9, 78]\\nA: ', '[35, -86, -38, 58, -9, 78]\\nConfidence: 95%']\n",
      "Question: ###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\n",
      "Q: [61, 35, -86, -38, 58, -9, 78]\n",
      "A: \n",
      "\n",
      "Answer: [35, -86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [03:22<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    chosenscos = scoretexts(ultrafeediff['chosen'][:50], model, 5)\n",
    "    rejscos = scoretexts(ultrafeediff['rejected'][:50], model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0096367e-554b-425c-88dc-5539da6b7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin_accs(chos, rejs):\n",
    "    rlast = [r[-1] for r in rejs]\n",
    "    clast = [c[-1] for c in chos]\n",
    "    return sum([clast[i]>rlast[i] for i in range(len(rejs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70e4187d-7ca5-4956-83b7-c1a8864de8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_accs(chosenscos, rejscos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685e5fe6-655d-4452-87ea-05ba12b2f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(chosenscos[i])>len(rejscos[i]) for i in range(len(rejscos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1295338-69b6-42d1-a9d3-18126818ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenscos = [[float(f) for f in chose] for chose in chosenscos]\n",
    "rejscos = [[float(f) for f in chose] for chose in rejscos]\n",
    "\n",
    "with open('boweven_ultrascos.pkl', 'wb') as file:\n",
    "    pickle.dump((chosenscos, rejscos), file)\n",
    "# with open('oasst_deberta_hhscores.pkl', 'wb') as file:\n",
    "#     pickle.dump((chosenscos, rejscos), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cd2c5e-33a3-4477-8377-bd4f316dbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('llama3b_hhscores.pkl', 'rb') as file:\n",
    "#     chosen, rejs = pickle.load((chosenscos, rejscos), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36eb5b0-d1b1-432d-b933-fd7bce895621",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenscos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a84edea-6a91-44ff-8190-609a61771549",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3b_hhscores.pkl', 'rb') as file:\n",
    "    l3bchos, l3rejs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e58b6663-6b74-4e82-9a36-8a40298a86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ultra13b.pkl', 'rb') as file:\n",
    "    ultrachos, ultrarejs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "720a0379-07e9-464d-81e4-91ee8122e57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bow_ultrascos.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbow_ultrascos.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     ultrachos, ultrarejs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlhfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bow_ultrascos.pkl'"
     ]
    }
   ],
   "source": [
    "with open('bow_ultrascos.pkl', 'rb') as file:\n",
    "    ultrachos, ultrarejs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0365917e-3a6a-4394-b20f-ec67c0021131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_accs(l3bchos, l3rejs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8e0d97c-4f03-479c-8f15-2fe0709b328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boweq = Dataset.load_from_disk(\"../../data/bagofwords/bowsynth100k/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "610c80f5-85f1-4a81-8ec8-edbb7813e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "otok = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a668e130-d3c8-4bcb-8820-b05633152c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlhfutils.rewards import get_synth_rewards\n",
    "\n",
    "def proc_eqlen(ex):\n",
    "    def cutstr(s, lim=1000):\n",
    "        ts = otok(s).input_ids[:lim]\n",
    "        return len(ts), otok.decode(ts, skip_special_tokens=True)\n",
    "    ex['stoks'], ex[\"question\"] = cutstr(ex['question'])\n",
    "    ex['tokj'], ex['response_j'] = cutstr(ex['response_j'])\n",
    "    ex['tok'], ex['response_k'] = cutstr(ex['response_k'])\n",
    "    resps = [ex['response_j'], ex['response_k']]\n",
    "    scos = get_synth_rewards(resps, \"bagofwords\")\n",
    "    jind = 0 if scos[0]>scos[1] else 0\n",
    "    ex['response_j'] = resps[jind]\n",
    "    ex['response_k'] = resps[1-jind]\n",
    "    ex['score_j'] = scos[jind]\n",
    "    ex['score_k'] = scos[1-jind]\n",
    "    ex['magnitude'] = ex['score_j'] - ex['score_k']\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0754f53a-e204-42ee-95a2-7a9c23c049eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "boweq = boweq.map(proc_eqlen, num_proc=10)\n",
    "# boweq = boweq.filter(lambda ex: (ex['stoks']==30) and (ex['tok']==30) and (ex['tokj']==30))\n",
    "print(len(boweq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4f0d1a3-c82c-43b6-a081-d07b36a61054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51432"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boweq.filter(lambda ex: ex['tokj']>ex['tok']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06fc0355-5f04-4620-9d5b-b23d45fb9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "boweq.save_to_disk(\"../../data/bagofwords/bowprefseqlenprefs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a7fc9-5aa7-4356-9c83-98cc17f7ca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
