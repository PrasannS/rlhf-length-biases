{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66644557-0a5d-4984-92dc-631aeb6ddca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasann/miniconda3/envs/rlenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-21 14:16:15,009] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# This is a notebook to do experiment requested by Nathan for checking token-wise length bias of rewards \n",
    "# (my guess: we'll see a lot at smaller scales, not so much at bigger)\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ab8ccb-1add-43e7-9e8f-23c5d5709b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "# UltraRM format so that we don't need to re-run a ton of times for each unique input\n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f09da245-ece8-48df-baf1-742d7f9319db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhrlhf_preproc(inp, out):\n",
    "    resp = inp\n",
    "    if \"###Human: \" not in inp: \n",
    "        resp = \"###Human: \"+inp\n",
    "    if \"###Assistant: \" not in out: \n",
    "        resp = resp+\" ###Assistant: \"\n",
    "    resp = resp+out\n",
    "    return resp\n",
    "    \n",
    "def progress_causalrm(inps, mod, tok, cache):\n",
    "    cache.clear()\n",
    "    inps.reverse()\n",
    "    scores = []\n",
    "    # TODO test to see if batching is needed, currently this assumes no batching\n",
    "    for i in tqdm(range(len(inps))):\n",
    "        #print(inps[i])\n",
    "        # switch batck to original HHRLHF format (TODO this is weird)\n",
    "        question, answer = inps[i]\n",
    "        question = question.replace(\"###Assistant:\", \"\\n\\nAssistant:\").replace(\"###Human:\", \"\\n\\nHuman:\")\n",
    "        question = (question + \"\\n\\nAssistant:\").strip()\n",
    "        tokrewards = None\n",
    "        # used cached full response if we can\n",
    "        for k in cache.keys():\n",
    "            if question in k: \n",
    "                tokrewards = cache[question]\n",
    "        modinps = tok(question+answer, return_tensors='pt')\n",
    "        \n",
    "        if tokrewards==None: \n",
    "            modinps = modinps.to(mod.device)\n",
    "            # first one in the cache\n",
    "            cache[question] = mod(**modinps).tolist()[0]\n",
    "            tokrewards = cache[question]\n",
    "        if i==0:\n",
    "            print(question+answer)\n",
    "            #print(len(modinps))\n",
    "            #print(len(tokrewards))\n",
    "            \n",
    "        # get the token score based on length of stuff\n",
    "        scores.append(tokrewards[len(modinps.input_ids[0])-1])   \n",
    "        #print(len(tokrewards))\n",
    "        #print(len(modinps.input_ids[0])-1)\n",
    "    scores.reverse()\n",
    "    return scores\n",
    "\n",
    "def progress_oasst(inps, mod, tok):\n",
    "    scores = []\n",
    "    # TODO test to see if batching is needed\n",
    "    for i in tqdm(range(len(inps))):\n",
    "        #print(inps[i])\n",
    "        # switch batck to original HHRLHF format (TODO this is weird)\n",
    "        question, answer = inps[i]\n",
    "        question = question.replace(\"###Assistant:\", \"\\n\\nAssistant:\").replace(\"###Human:\", \"Human:\")\n",
    "        # question = (question + \"\\n\\nAssistant:\").strip()\n",
    "        question = question.strip()+\"\\n\\nAssistant: \"\n",
    "        if i==0:\n",
    "            print(question, answer)\n",
    "        modinps = tokenizer(question, answer, return_tensors='pt').to(mod.device)\n",
    "        scores.append(mod(**modinps).logits[0].cpu().detach())\n",
    "    return scores\n",
    "\n",
    "def load_rm(rmname): \n",
    "    # TODO maybe offload input formatting into here as well\n",
    "    if rmname==\"weqweasdas/hh_rlhf_rm_open_llama_3b\":\n",
    "        rm_tokenizer = AutoTokenizer.from_pretrained(\"weqweasdas/hh_rlhf_rm_open_llama_3b\")\n",
    "        rm_pipe = pipeline(\n",
    "          \"sentiment-analysis\",\n",
    "          model=\"weqweasdas/hh_rlhf_rm_open_llama_3b\",\n",
    "          device=1,\n",
    "          tokenizer=rm_tokenizer,\n",
    "          model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    "        )\n",
    "        pipe_kwargs = {\n",
    "          \"return_all_scores\": True,\n",
    "          \"function_to_apply\": \"none\",\n",
    "          \"batch_size\": 4\n",
    "        }\n",
    "        def progress_pipe(inps, kwargs, pipe): \n",
    "            chunk=16\n",
    "            results = []\n",
    "            print(inps[0])\n",
    "            for i in tqdm(range(0, len(inps), chunk)):\n",
    "                results.extend(pipe(inps, **kwargs))\n",
    "            return results\n",
    "        return lambda texts: [output[0][\"score\"] for output in progress_pipe([hhrlhf_preproc(i, o) for i, o in texts], pipe_kwargs, rm_pipe)]\n",
    "    if rmname==\"OpenAssistant/reward-model-deberta-v3-large-v2\":\n",
    "        rank_model, toker = AutoModelForSequenceClassification.from_pretrained(rmname, device_map=1), AutoTokenizer.from_pretrained(rmname)\n",
    "        return lambda texts: progress_oasst(texts, rank_model, toker)\n",
    "    if rmname==\"openbmb/UltraRM-13b\":\n",
    "        # load in models\n",
    "        rtoker = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\")\n",
    "        rmodel = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\", device_map=1, torch_dtype=torch.bfloat16)\n",
    "        rmodel.eval()\n",
    "        # cache full rewards (tokenwise) for the longest seqeunce (reverse the passing order), then we can pull from that \n",
    "        reward_cache = {}\n",
    "        \n",
    "        return lambda texts: progress_causalrm(texts, rmodel, rtoker, reward_cache)\n",
    "\n",
    "def get_token_inps(inputs, toker, gap=1, flat=True):\n",
    "    oldinps = None\n",
    "    # only work with output part of input tuples (TODO this may need adaptation for sanity check)\n",
    "    if len(inputs[0])==2: \n",
    "        oldinps = inputs\n",
    "        inputs = [inputs[i][1] for i in range(len(inputs))]\n",
    "    finlist = []\n",
    "    toklists = [toker(inp).input_ids for inp in inputs]\n",
    "    for toks in toklists: \n",
    "        tmp = []\n",
    "        for ind in range(gap, len(toks), gap): \n",
    "            tmp.append(toker.decode(toks[:ind+1], skip_special_tokens=True))\n",
    "        if (len(toks)%gap)!=0:\n",
    "            tmp.append(toker.decode(toks, skip_special_tokens=True))\n",
    "        finlist.append(tmp)\n",
    "    if oldinps: \n",
    "        # we want something we can use directly\n",
    "        if flat: \n",
    "            rlens = [len(finlist[i]) for i in range(len(finlist))]\n",
    "            inps = []\n",
    "            outs = []\n",
    "            for i in range(len(oldinps)): \n",
    "                inps.extend([oldinps[i][0]]*rlens[i])\n",
    "                outs.extend(finlist[i])\n",
    "            rlens = [sum(rlens[:i+1]) for i in range(len(rlens))]\n",
    "            return [0]+rlens, [(inps[i], outs[i]) for i in range(len(outs))]\n",
    "        # get the corrected reprocessed thing\n",
    "        return [(oldinps[i][0], finlist[i]) for i in range(len(finlist))]\n",
    "    return finlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3bd8b5-7906-435b-a5d8-76045789f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_hh(ex): \n",
    "    ex['chosen'] = ex['chosen'].replace(\"\\n\\n\", \" ###\")\n",
    "    ex['rejected'] = ex['rejected'].replace(\"\\n\\n\", \" ###\")\n",
    "    return ex\n",
    "\n",
    "def pp_uf(ex):\n",
    "    ex['chosen'] = \"###Human: \"+ex['question'].strip()+\" ###Assistant: \"+ex['response_j']\n",
    "    ex['rejected'] = \"###Human: \"+ex['question'].strip()+\" ###Assistant: \"+ex['response_k']\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e164e18-f041-44d6-8afa-f2da9da7a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/prasann/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-37c6f75e35564d2a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading cached processed dataset at /home/prasann/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-37c6f75e35564d2a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a0c4ef1f168e4156_*_of_00010.arrow\n"
     ]
    }
   ],
   "source": [
    "# Code for loading in data\n",
    "hh_train = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"test\").map(pp_hh, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f88216-1a89-45ec-a179-1533e59e689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /data/users/prasann/Projects/rlhf-length-biases/data/ultrafeeddiff/cache-3c77a68b828b795c.arrow\n",
      "Loading cached processed dataset at /data/users/prasann/Projects/rlhf-length-biases/data/ultrafeeddiff/cache-78f11653849498af.arrow\n",
      "Loading cached processed dataset at /data/users/prasann/Projects/rlhf-length-biases/data/ultrafeeddiff/cache-79fd4cb5fb1100dc.arrow\n"
     ]
    }
   ],
   "source": [
    "ultrafeediff = Dataset.load_from_disk(\"../../data/ultrafeeddiff/\").shuffle(seed=0).select(range(1000)).filter(lambda ex: ex['tokj']<150).map(pp_uf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0ab35-91ad-4de9-97f2-060f9f7bcd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultrafeediff['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f4e232-c371-4aeb-b6b1-464dc9cdb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardmodels = [\"OpenAssistant/reward-model-deberta-v3-large-v2\", \"weqweasdas/hh_rlhf_rm_open_llama_3b\", \"openbmb/UltraRM-13b\"]\n",
    "rmstr = rewardmodels[0]\n",
    "# TODO maybe we should use the same tokenizer across all of these? Aligning between models is gonna be a bit weird\n",
    "tokenizer = AutoTokenizer.from_pretrained(rewardmodels[0])\n",
    "model = load_rm(rmstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff8ea12e-ed3c-406a-8e1b-1f8547ee044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoretexts(texts, mod, gap=5): \n",
    "    test_inps = [x.rsplit(\"###Assistant: \", 1) for x in texts]\n",
    "    print(test_inps[0])\n",
    "    values, data = get_token_inps(test_inps, tokenizer, gap)\n",
    "    scos = model(data)\n",
    "    nscos = [scos[values[i-1]:values[i]] for i in range(1, len(values))]\n",
    "    return nscos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b8b2fa-e22a-4412-9a9c-f44f05305bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\\nQ: [61, 35, -86, -38, 58, -9, 78]\\nA: ', 'The filtered list is: [61, 35, -86, -38, 58, -9]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\n",
      "Q: [61, 35, -86, -38, 58, -9, 78]\n",
      "A:\n",
      "\n",
      "Assistant:  The filtered list is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 450/450 [00:23<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\\nQ: [61, 35, -86, -38, 58, -9, 78]\\nA: ', '[35, -86, -38, 58, -9, 78]\\nConfidence: 95%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                            | 4/991 [00:00<00:28, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: In this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3.\n",
      "Q: [61, 35, -86, -38, 58, -9, 78]\n",
      "A:\n",
      "\n",
      "Assistant:  [35, -86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 991/991 [01:15<00:00, 13.21it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    chosenscos = scoretexts(ultrafeediff['chosen'][:50], model, 5)\n",
    "    rejscos = scoretexts(ultrafeediff['rejected'][:50], model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0096367e-554b-425c-88dc-5539da6b7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin_accs(chos, rejs):\n",
    "    rlast = [r[-1] for r in rejs]\n",
    "    clast = [c[-1] for c in chos]\n",
    "    return sum([clast[i]>rlast[i] for i in range(len(rejs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70e4187d-7ca5-4956-83b7-c1a8864de8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_accs(chosenscos, rejscos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685e5fe6-655d-4452-87ea-05ba12b2f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(chosenscos[i])>len(rejscos[i]) for i in range(len(rejscos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1295338-69b6-42d1-a9d3-18126818ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenscos = [[float(f) for f in chose] for chose in chosenscos]\n",
    "rejscos = [[float(f) for f in chose] for chose in rejscos]\n",
    "\n",
    "with open('deberta_openassist_ufdata.pkl', 'wb') as file:\n",
    "    pickle.dump((chosenscos, rejscos), file)\n",
    "# with open('oasst_deberta_hhscores.pkl', 'wb') as file:\n",
    "#     pickle.dump((chosenscos, rejscos), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cd2c5e-33a3-4477-8377-bd4f316dbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('llama3b_hhscores.pkl', 'rb') as file:\n",
    "#     chosen, rejs = pickle.load((chosenscos, rejscos), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36eb5b0-d1b1-432d-b933-fd7bce895621",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenscos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a84edea-6a91-44ff-8190-609a61771549",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3b_hhscores.pkl', 'rb') as file:\n",
    "    l3bchos, l3rejs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e58b6663-6b74-4e82-9a36-8a40298a86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ultra13b.pkl', 'rb') as file:\n",
    "    ultrachos, ultrarejs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0365917e-3a6a-4394-b20f-ec67c0021131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_accs(l3bchos, l3rejs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0d97c-4f03-479c-8f15-2fe0709b328e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
